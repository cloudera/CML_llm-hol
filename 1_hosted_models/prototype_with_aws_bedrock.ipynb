{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04ea027-3a19-42cd-bae4-1780112d9d29",
   "metadata": {},
   "source": [
    "# 1. Prototype With AWS Bedrock Foundational Models\n",
    "Welcome to the first exercise in the CML Hands on Lab. In this notebook you will get familiar with calling an externally hosted foundational model. For this exercise we will be using AWS Bedrock service and the Anthropic Claude model hosted there.\n",
    "\n",
    "![image](../assets/jupypter-session-bedrock.png)\n",
    "\n",
    "### 1.1 Imports and AWS Bedrock client setup\n",
    "Of note here is `boto3` SDK to interact with AWS services. The `get_bedrock_client` function is from AWS's [github repository](https://github.com/aws-samples/amazon-bedrock-workshop/blob/109ed616fd14c9eb26eda9bef96eb78c490d5ef6/utils/bedrock.py#L13). If you are running this code in your own environment, make sure to set AWS keys, preferably as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58bc9967-bc2d-4ef5-a50d-9c8a27ded0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "if os.environ.get(\"AWS_ACCESS_KEY_ID\") == \"\":\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<YOUR-ACCESS-KEY-ID>\"   # Replace this if running in your own environment\n",
    "\n",
    "if os.environ.get(\"AWS_SECRET_ACCESS_KEY\") == \"\":\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<YOUR-SECRET-ACCESS-KEY>\"   # Replace this if running in your own environment\n",
    "\n",
    "# TODO: for a lab, can reduce some of the checks in the below function\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    endpoint_url: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    endpoint_url :\n",
    "        Optional override for the Bedrock service API Endpoint. If setting this, it should usually\n",
    "        include the protocol i.e. \"https://...\"\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if endpoint_url:\n",
    "        client_kwargs[\"endpoint_url\"] = endpoint_url\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c45244-202e-487b-8134-a49f84c57b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then the client is initialized, binding to AWS region where Bedrock service is available. [As of October 2023](https://aws.amazon.com/about-aws/whats-new/2023/10/amazon-bedrock-asia-pacific-tokyo-aws-region/), these regions are us-east-1, us-west-2, and ap-northeast-1. We'll be using `us-east-1` as the default. This can be overwritten with an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8444b23e-f063-4923-b12e-54e453529ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "# Initializing the bedrock client using AWS credentials\n",
    "# If you are using a special Assumed role or custom endpoint url, see get_bedrock_client\n",
    "if os.environ.get(\"AWS_DEFAULT_REGION\") == \"\":\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "      region=os.environ.get(\"AWS_DEFAULT_REGION\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3118c5-e310-4748-96d8-61f2016deab8",
   "metadata": {},
   "source": [
    "### 1.3 Set desired instruction: Text Summarization\n",
    "The bedrock model shown in this notebook (Anthropic's Claude) is a general instruction-following text generation model. Meaning we can provide some instructions and input text to generate a response that will follow the instructions provided. As an example, we will provide instruction to the foundational model to summarize, in a few bullet points, a chunk of a text.  Model instructions typically follow a prescribed pattern and depend on the model used. In other words, the is no standard way to provide insturctions to different models. Below we follow [Anthropic's suggested structure](https://docs.anthropic.com/claude/docs/constructing-a-prompt). For example, note the use of keywords `Human:` and `Assistant:`. These are specific to the Claude foundational model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09516a0-a82d-4101-a33d-0e101371bcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction_text = \"\"\"Human: Please provide a summary of the text inside <text></text> XML tags. Do not add any information that is not mentioned in this text. \n",
    "                             Provide no more than 3 bullet points in the summary, each being a complete sentece. \n",
    "                             Start your summary with simply saying \"Here's a brief summary of the provided text:\". \n",
    "                    <text>{{USER_TEXT}}</text>\n",
    "                    Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a9f24-193e-4afa-b203-f0d6e4a7bcd6",
   "metadata": {},
   "source": [
    "### 1.4 Set input text and create complete prompt <a id='1.4'></a>\n",
    "\n",
    "This is the input text that we want to be summarized. The length of this text plus any included instructions must fit within the context window size of the selected model. For Claude it's approximately 9,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06fb50d0-4710-485c-8cc8-94503d868e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = '''Machine learning has become one of the most critical capabilities for modern businesses to grow and stay competitive today. From automating internal processes to optimizing the design, creation, and marketing processes behind virtually every product consumed, ML models have permeated almost every aspect of our work and personal lives.\n",
    "ML development is iterative and complex, made even harder because most ML tools aren’t built for the entire machine learning lifecycle. Cloudera Machine Learning on Cloudera Data Platform accelerates time-to-value by enabling data scientists to collaborate in a single unified platform that is all inclusive for powering any AI use case. Purpose-built for agile experimentation and production ML workflows, Cloudera Machine Learning manages everything from data preparation to MLOps, to predictive reporting. Solve mission critical ML challenges along the entire lifecycle with greater speed and agility to discover opportunities which can mean the difference for your business.\n",
    "Each ML workspace enables teams of data scientists to develop, test, train, and ultimately deploy machine learning models for building predictive applications all on the data under management within the enterprise data cloud. ML workspaces support fully-containerized execution of Python, R, Scala, and Spark workloads through flexible and extensible engines.'''\n",
    "\n",
    "# Replace instruction placeholder to build a complete prompt\n",
    "full_prompt = instruction_text.replace(\"{{USER_TEXT}}\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a5494-4c00-4f8e-b5a3-22d7b1fd4a2b",
   "metadata": {},
   "source": [
    "### 1.5 Creating API request for Titan model\n",
    "With the prompt out of the way, we generate a JSON payload to send to Bedrock for processing. The parameters and format required for this API request is specific to the Titan model, see AWS Bedrock documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d66c113-5b2b-4085-a386-783a09c8a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results successfully retreived\n"
     ]
    }
   ],
   "source": [
    "# Model expects a JSON object with a defined schema\n",
    "body = json.dumps({\"prompt\": full_prompt,\n",
    "             \"max_tokens_to_sample\":4096,\n",
    "             \"temperature\":0.6,\n",
    "             \"top_k\":250,\n",
    "             \"top_p\":1.0,\n",
    "             \"stop_sequences\":[]\n",
    "              })\n",
    "\n",
    "# Provide a model ID and call the model with the JSON payload\n",
    "modelId = 'anthropic.claude-v2:1'\n",
    "response = boto3_bedrock.invoke_model(body=body, modelId=modelId, accept='application/json', contentType='application/json')\n",
    "response_body = json.loads(response.get('body').read())\n",
    "print(\"Model results successfully retreived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c529eff-f072-404b-9be0-d3e06b283a9a",
   "metadata": {},
   "source": [
    "### 1.6 Review the results\n",
    "The response body is specific to the Claude Model API, see AWS Bedrock documentation for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcfb28b2-2874-40f4-adb8-9c09b9905a04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's a brief summary of the provided text:\n",
      "\n",
      "- Machine learning models have become critical for modern businesses to stay competitive and grow.\n",
      "- ML development is complex and iterative, made harder because most ML tools aren’t built for the entire machine learning lifecycle. \n",
      "- Cloudera Machine Learning aims to accelerate time-to-value by enabling collaboration on a unified platform for the entire ML lifecycle.\n"
     ]
    }
   ],
   "source": [
    "result = response_body.get('completion')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ee10b-28e8-4bbd-93cc-0e7e40339c74",
   "metadata": {},
   "source": [
    "**(BONUS)** Go back to [step 1.4](#1.4) and paste a different text for the model to summarize. See how it does on the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f15aa4-80df-4b23-83f3-519a493c6b32",
   "metadata": {},
   "source": [
    "### 1.7 Takeaways\n",
    "\n",
    "* [Cloudera Machine Learning](https://docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.html#cdsw_overview) provides a flexible environment to integrate with 3rd party foundational models\n",
    "* JupyterLabs is a supported editor, along with other editors that can be optinoally added to [custom runtimes](https://docs.cloudera.com/machine-learning/cloud/runtimes/topics/ml-creating-a-customized-runtimes-image.html) (e.g. RStudio, VSCode)  \n",
    "* Users can prototype LLM solutions quickly, using development tools they are most efficient with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84059805-302c-4e2d-9035-bc6366954c48",
   "metadata": {},
   "source": [
    "### Up Next: Go to Exercise 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
