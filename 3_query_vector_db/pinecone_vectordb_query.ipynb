{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working With Pinecone Vector Database\n",
    "Although Vector Databases have existed long before Large Language Models, vector DBs have become an important part of many LLM solutions. In particular, Retreival Augmented Generation (or RAG) architecture addresses LLM's halucinations and issues with longer-term memory by augmenting the user's prompt with the results of a search accross a vector DB. [Pinecone](https://www.pinecone.io/) is a cloud-based vector database that is easy to integrate with your CML workflow, as this notebook shows. \n",
    "\n",
    "Recall that in the previous exervice you created CML jobs to scrape a site and load each page into Pinecone vector DB. This notebook will focus on interacting with Pinecone from a Jupyter notebook.\n",
    "\n",
    "![Exercise 3 overview](../assets/exercise_3.png)\n",
    "\n",
    "### 3.1 Imports and global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "PINECONE_API_KEY = '483fc5b2-b428-4a98-b11b-37a1c692e6ff' #os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n",
    "\n",
    "PINECONE_INDEX = os.getenv('PINECONE_INDEX')\n",
    "dimension = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Pinecone connection\n",
    "Pinecone client is initialized with the parameters defined previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Pinecone connection...\n",
      "Pinecone initialised\n"
     ]
    }
   ],
   "source": [
    "print(\"initialising Pinecone connection...\")\n",
    "#pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(\"Pinecone initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 'cml-index-se-west' as object...\n",
      "Success\n",
      "Total number of embeddings in Pinecone index is 25.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting '{PINECONE_INDEX}' as object...\")\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "print(\"Success\")\n",
    "\n",
    "# Get latest statistics from index\n",
    "current_collection_stats = index.describe_index_stats()\n",
    "print('Total number of embeddings in Pinecone index is {}.'.format(current_collection_stats.get('total_vector_count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Function to peform the vector search \n",
    "The idea is to find a chunk from the Knowledge Base that is \"close\" to what the original user's prompt is. We perform a semantic search using the user's question, find the nearest knowledge base chunk, and return the content of that chunk along with its source and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x7f6ac030c460>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get embeddings for a user question and query Pinecone vector DB for nearest knowledge base chunk\n",
    "def get_nearest_chunk_from_pinecone_vectordb(index, question):\n",
    "    # Generate embedding for user question with embedding model\n",
    "    retriever = SentenceTransformer(EMBEDDING_MODEL_REPO)\n",
    "    xq = retriever.encode([question]).tolist()\n",
    "    xc = index.query(vector=xq, top_k=5,\n",
    "                 include_metadata=True)\n",
    "    \n",
    "    matching_files = []\n",
    "    scores = []\n",
    "    for match in xc['matches']:\n",
    "        # extract the 'file_path' within 'metadata'\n",
    "        file_path = match['metadata']['file_path']\n",
    "        # extract the individual scores for each vector\n",
    "        score = match['score']\n",
    "        scores.append(score)\n",
    "        matching_files.append(file_path)\n",
    "\n",
    "    # Return text of the nearest knowledge base chunk \n",
    "    # Note that this ONLY uses the first matching document for semantic search. matching_files holds the top results so you can increase this if desired.\n",
    "    response = load_context_chunk_from_data(matching_files[0])\n",
    "    sources = matching_files[0]\n",
    "    score = scores[0]\n",
    "    return response, sources, score\n",
    "\n",
    "# Return the Knowledge Base doc based on Knowledge Base ID (relative file path)\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examine the results of the vector search\n",
    "Given the text of the question, we can now perform a vector search and output the results in the notebook. An important detail here is the ability to interact with metadata (e.g. context source) which can be used to narrow down the search space and, more critically, for authorization. These approaches are out of scope of this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context Chunk: \n",
      "Best practices for building Apache Spark applicationsCloudera Docs Best practices for building Apache Spark applications   Follow these best practices when building Apache Spark Scala and Java       applications:   Refrain from using withColumn in chain, loop, and calling it multiple times in a single         query. Doing so may cause performance issues. To avoid issues, use select() with multiple         columns at once. See the Apache Spark API reference linked below for more information.  Compile your applications against the same version of Spark that         you are running.                Build a single assembly JAR (\"Uber\" JAR) that includes all dependencies. In Maven, add             the Maven assembly plug-in to build a JAR containing all dependencies:            <plugin>   <artifactId>maven-assembly-plugin</artifactId>   <configuration>     <descriptorRefs>       <descriptorRef>jar-with-dependencies</descriptorRef>     </descriptorRefs>   </configuration>   <executions>     <execution>       <id>make-assembly</id>       <phase>package</phase>       <goals>         <goal>single</goal>       </goals>     </execution>   </executions> </plugin>  This plug-in manages the merge procedure for all available JAR           files during the build. Exclude Spark, Hadoop, and Kafka classes from           the assembly JAR, because they are already available on the cluster           and contained in the runtime classpath. In Maven, specify Spark,           Hadoop, and Kafka dependencies with scope provided.           For example:  <dependency>   <groupId>org.apache.spark</groupId>   <artifactId>spark-core_2.11</artifactId>   <version>2.4.0.7.0.0.0</version>   <scope>provided</scope> </dependency>    \n",
      "\n",
      "Context Source(s): \n",
      "/home/cdsw/data/https:/docs.cloudera.com/data-engineering/cloud/best-practices/topics/spark-best-practices-building-apps.txt\n",
      "\n",
      "Pinecone Score: \n",
      "0.187403232\n"
     ]
    }
   ],
   "source": [
    "#question = \"What is ML Runtime?\" ## (Swap with your own based on your dataset)\n",
    "question = \"can I use withColumn in chain?\"\n",
    "#question = \"how many states does the us have?\"\n",
    "context_chunk, sources, score = get_nearest_chunk_from_pinecone_vectordb(index, question)\n",
    "print(\"\\nContext Chunk: \")\n",
    "print(context_chunk)\n",
    "print(\"\\nContext Source(s): \")\n",
    "print(sources)\n",
    "print(\"\\nPinecone Score: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Takeaways\n",
    "* Vector search is a critical component of any LLM app using RAG architecture\n",
    "* Cloudera's partner [Pinecone](https://www.pinecone.io/) provides a convenient SaaS offering for a Vector Database to support LLM RAG architecture\n",
    "* Metadata from each entry in the Vector DB can be used to refine searches and add custom authorization frameworks\n",
    "\n",
    "### Up Next: Go to Exercise 4\n",
    "Where a gradio app is launched to complete the first iteration of the Q&A chat use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
